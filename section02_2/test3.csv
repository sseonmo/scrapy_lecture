contents
" is a high-level API for headless chrome. It’s one of the most popular tools to use for web automation or web scraping in Node.js. In web scraping, many developers use it to handle javascript rendering and web data extraction. In this article, we are going to cover how to set up a proxy in Puppeteer and what your options are if you want to rotate proxies.In this section, we’re going to configure Puppeteer to use a proxy. For this, you will need a working proxy and a destination URL to send the request to.As simple as that. This code will ensure that every request goes through the defined proxy. One downside with Puppeteer is that you cannot define proxies for each request in a simple way. So, the specified proxy will be used for all the requests of the browser instance.When you scrape the web at scale, you need to  to avoid bans. If you want to implement your own IP pool in Puppeteer you will realize that you can only set up proxies on browser-level (code above) and not per request. This is not ideal if you need to use different proxies for each request. "
"2019 was an exciting year for Scrapinghub. We created things we have never created before and did things nobody in our industry had ever done before. Let’s revisit what happened in 2019!We organized the first industry event focused on  in Dublin Ireland. With more than 140 attendees, 16 speakers covering topics from technical deep dives to business use cases, 12 presentations, a customer panel discussion, and unlimited Guinness. It was an absolute success.It was an awesome event. We loved it and the people who attended loved it, with 94% of attendees giving it an overall rating of Excellent/Good. So we are going to do it again! We’re going to make sure Extract Summit 2020 is going to be even bigger and better. Watch out for our call for speakers which we will launch in February.We launched AutoExtract API our "
"Attracting top talent is essential for the success and growth of a company. The majority of employers will agree that finding the best talent is just as hard as it is important. Which is why, rather than waiting for the right candidate to magically fall into your lap, it's time for employers to turn towards the untapped power of .In this blog, I try to identify the problems that the talent acquisition industry is facing and how to best solve them.Maybe we don’t really have a skills shortage, it's more that we are looking for the right people in the wrong location.For many roles, you don’t need to be in a specific location in order to carry out your job. Software developers, for example, can carry out their job responsibilities from anywhere as long as they have access to the necessary hardware and tools to do the job. The same can be said for many other job roles. Yet many companies will continue to search for the same type of talent as they always have - people who live (or are willing to live) near the “office” and who work 5 days a week."
"One of the biggest pain points we’ve heard from our Crawlera customers last year is the inconvenience of having to jump from one Crawlera plan to another, when more requests are needed in a month. For this reason, we have been working on rethinking our Crawlera plans to better accommodate these cases and be more flexible with customers that have variable crawling requirements from month to month.Today, we are introducing a new group of  that allow you to go over your monthly quota without the hassle of upgrading and downgrading plans. You stay on a single plan and pay for overages, only when you incur them. You will also be able to limit the amount of overages you can incur into, to protect against unexpected expenses.We are also simplifying the number of plans to just three:These new plans replace the old Crawlera “C plans” (C10, C50, C100, C200) which are no longer available to new customers. If you are already subscribed to an old plan, nothing changes for you at this time. We will notify you if there is a change that will impact your plan."
"On February 9th, 2020, Ireland elected a new parliament. Prior to the elections, the political parties invested a lot of time, money and energy to get their political message to the people. A lot of research goes into selecting the right platform and the right medium.In recent years, social media has gained importance, however, traditional newspaper coverage is still crucial to political parties to get their messages across. Which political party is achieving the widest newspaper coverage? Does the coverage correspond with the political parties' voter shares? These are questions of interest for the political parties which strive for more newspaper coverage, as well as for the voters as they want to know whether their preferred newspaper is biased.In this post, we investigate the impact of news media coverage on the results of the Irish general election. To that end, more than 200,000 newspaper articles are analyzed to find the mentions of political parties. We consider ‘mentions’ as an important indicator because it estimates the overall space that media houses give to the political parties.Building on this we first investigate the correlation between the media coverage and the vote share. Next, we examine coverage bias by analyzing the political coverage across different media houses of Ireland. Finally, based on our analysis we determine if specific media houses are better suited to predict the trends of the Irish elections.Steps we followed for our analysis:"
"Web scraping is when you extract data from the web and put it in a structured format. Getting structured data from publicly available websites and pages should not be an issue as everybody with an internet connection can access these websites. You should be able to structure it as well. In reality though, it’s not that easy.One of the main use cases of web scraping is in the e-commerce world: price monitoring and . However, you can also use web scraping for , , "
"As a python developer at Scrapinghub, I spend a lot of time in the Scrapy shell. This is a command-line interface that comes with Scrapy and allows you to run simple, spider compatible code. It gets the job done, sure, but there’s a point where a command-line interface can become kinda fiddly and I found I was passing that point pretty regularly. I have some background in tool design and task automation so I naturally began to wonder how I could improve my experience and help focus more time on building great spiders. Over my Christmas break, I dedicate some free time to improve this experience, resulting in my first python package .Scrapy-GUI offers two different UI tools to help build Scrapy spiders, but today I am going to focus on the part that integrates directly into a Scrapy shell - its  method.First things first, the installation part is simple, as it is on the Python Package Index. All you need to do is install it via pip. "
"We’re excited to announce our newest data extraction API, . From now on, you can use AutoExtract to extract Job Postings data from many job boards and recruitment sites. Without writing any custom data extraction code!Aside from e-commerce products and news extraction, one of the most demanded web data types is job postings. Job Postings API enables you to get real-time job postings data, at scale.If you want to learn more about the activities of fortune 100/500/1000 companies, looking at their job postings could be an essential element of your research. It can indicate what direction the companies are heading and give you outside insights on what technologies they are investing in. Job postings can also be a great addition to your business intelligence (BI) data sources.By monitoring job postings, you can also get a clear understanding of what direction your competitors, partners or suppliers are moving. Getting access to job postings data can help you determine what markets your competitors are going after."
"The Internet offers a in the form of , news, blog posts, stories, essays, tutorials that can be leveraged by many useful applications:But anyone interested in using all this data available, will face some challenges. Web pages are built of many components (menus, sidebars, ads, etc) and only a few of them represent the true article content, the actual valuable information. Being able to "
"Scrapinghub is a fully distributed organization with a remote workforce spread across the globe. This structure will enable us to continue to operate at full capacity during the Coronavirus pandemic and deliver full service to our customers.Businesses all over the world are trying to adapt to the new circumstances brought on by the Coronavirus such as being forced to implement a remote working environment while retaining productivity, a huge challenge for companies that normally don’t work remotely.We have had a lot of queries from our customers, who are doing internal global risk assessments on their supply chains being affected by COVID-19 so want to share our continued commitment to providing our customers with ongoing services during this time while ensuring a safe environment for all of our employees.Our internal risk models predict no more than 2% leave as a worst-case scenario, only slightly above baseline. This is likely to be offset by people deferring normal vacation leave due to travel limitations.Scrapinghub is at exceptionally low risk from any disruption in supplying our customers from the pandemic for two reasons:"
